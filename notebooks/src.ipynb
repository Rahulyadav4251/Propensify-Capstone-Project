{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pandas pandas-profiling scikit-learn sagemaker openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://kh3-ls-storage.s3.us-east-1.amazonaws.com/Updated Project guide data set/Propensify.zip\"  # Replace with the data URL\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    \n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        zip_ref.extractall(\"../data/propensify\")  \n",
    "    \n",
    "    train_df = pd.read_excel(\"../data/propensify/train.xlsx\") # historical data\n",
    "    test_df = pd.read_excel(\"../data/propensify/test.xlsx\")  # unseen data\n",
    "\n",
    "    # Display the DataFrames shape to verify\n",
    "    print(train_df.shape, test_df.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download file, status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the training and testing datasets to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the files locally\n",
    "train_df.to_csv(\"../data/train.csv\", index=False)\n",
    "test_df.to_csv(\"../data/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the files to S3\n",
    "train_path = session.upload_data(\n",
    "    path=\"../data/train.csv\", bucket=bucket, key_prefix=\"sagemaker/propensify\"\n",
    ")\n",
    "\n",
    "test_path = session.upload_data(\n",
    "    path=\"../data/test.csv\", bucket=bucket, key_prefix=\"sagemaker/propensify\"\n",
    ")\n",
    "\n",
    "print(f\"Train path: {train_path}\")\n",
    "print(f\"Test path: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit the Pipeline Model on Sagemaker!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define custom preprocessing functions\n",
    "\n",
    "def treat_missing_values(X):\n",
    "    ## Keeping only those columns that are required\n",
    "    columns_to_keep = ['custAge', 'profession', 'marital', 'schooling', 'default', 'housing',\n",
    "                       'loan', 'contact', 'month', 'day_of_week', 'campaign', 'pdays', 'previous',\n",
    "                       'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n",
    "                       'euribor3m', 'nr.employed', 'pmonths', 'pastEmail', 'responded']\n",
    "    \n",
    "    X = X[columns_to_keep]\n",
    "\n",
    "    ## Feature engineering for schooling\n",
    "    schooling_category = {\n",
    "        'basic.4y' : 'basic',\n",
    "        'basic.6y' : 'basic',\n",
    "        'basic.9y' : 'basic',\n",
    "        'high.school': 'high.school',\n",
    "        'illiterate':'illiterate',\n",
    "        'professional.course': 'professional.course',\n",
    "        'university.degree':'university.degree',\n",
    "        'unknown':'unknown',\n",
    "    }\n",
    "\n",
    "    X.loc[:,'schooling'] = X['schooling'].replace(schooling_category)\n",
    "\n",
    "    ## Imputation of missing values in education based on profession\n",
    "    imputation_mapping = {\n",
    "        'blue-collar' : 'basic',\n",
    "        'self-employed': 'illiterate',\n",
    "        'technician'   : 'professional.course',\n",
    "        'admin.'        : 'university.degree',\n",
    "        'services'      : 'high.school',\n",
    "        'management'    : 'university.degree',\n",
    "        'retired'       : 'unknown',\n",
    "        'entrepreneur'  : 'university.degree'\n",
    "    }\n",
    "\n",
    "    X.loc[:,'schooling'] = X['schooling'].combine_first(X['profession'].map(imputation_mapping))\n",
    "\n",
    "    ## Imputing age values\n",
    "    ## Calculate median age for each profession\n",
    "    median_age = X.groupby('profession')['custAge'].median().rename('mean_age').reset_index()\n",
    "\n",
    "    ## Create a mapping from profession to mean age\n",
    "    median_age_dict = median_age.set_index('profession')['mean_age'].to_dict()\n",
    "\n",
    "    ## Fill missing age values based on profession\n",
    "    X['custAge']  = X['custAge'].fillna(X['profession'].map(median_age_dict))\n",
    "\n",
    "    ## Impute random day for missing 'day_of_week' values\n",
    "    X.loc[:,'day_of_week'] = X['day_of_week'].apply(lambda day: np.random.choice(['mon', 'tue', 'wed', 'thu', 'fri']) if pd.isna(day) else day)\n",
    "\n",
    "    ## Drop remaining missing values\n",
    "    X = X.dropna()\n",
    "\n",
    "    return X\n",
    "\n",
    "def label_encoding(X):\n",
    "     ## Label encoding for 'profession'\n",
    "    X.loc[:,'profession'] = X['profession'].map({'student': 'Dependents', 'retired': 'Dependents', 'unemployed': 'Unemployed&Unknown', 'unknown': 'Unemployed&Unknown',\n",
    "                                                 'admin.': 'Working', 'blue-collar': 'Working', 'entrepreneur': 'Working', 'housemaid': 'Working',\n",
    "                                                 'management': 'Working', 'self-employed': 'Working', 'services': 'Working', 'technician': 'Working'})\n",
    "\n",
    "    ## Label encoding for 'marital'\n",
    "    X.loc[:,'marital'] = X['marital'].map({'single': 'Single&Divorced', 'divorced': 'Single&Divorced', 'married': 'married', 'unknown': 'Unknown'})\n",
    "\n",
    "    ## Label encoding for 'schooling'\n",
    "    X.loc[:,'schooling'] = X['schooling'].map({'basic': 'Uneducated&BasicEducation', 'high.school': 'Uneducated&BasicEducation',\n",
    "                                               'illiterate': 'Uneducated&BasicEducation', 'unknown': 'Unknown',\n",
    "                                               'professional.course': 'Educated', 'university.degree': 'Educated'})\n",
    "\n",
    "    ## Transforming month to quarter in a new column\n",
    "    ## Create a dictionary mapping  month names to quarters\n",
    "    month_to_quarter = {\n",
    "        'jan': 'Q1', 'feb': 'Q1', 'mar': 'Q1',\n",
    "        'apr': 'Q2', 'may': 'Q2', 'jun': 'Q2',\n",
    "        'jul': 'Q3', 'aug': 'Q3', 'sep': 'Q3',\n",
    "        'oct': 'Q4', 'nov': 'Q4', 'dec': 'Q4'\n",
    "    }\n",
    "\n",
    "    ## Map the  month names to quarters\n",
    "    X['quarter'] = X['month'].map(month_to_quarter)\n",
    "\n",
    "    ## Dropping month column\n",
    "    X = X.drop(columns='month', axis=1)\n",
    "\n",
    "    ## Label encoding for 'day_of_week'\n",
    "    X.loc[:,'day_of_week'] = X['day_of_week'].map({'mon': 'WeekBeginning', 'tue': 'WeekBeginning', 'wed': 'WeekBeginning',\n",
    "                                                   'thu': 'WeekEnding', 'fri': 'WeekEnding'})\n",
    "    \n",
    "    ## Label encoding for 'default'\n",
    "    X.loc[:,'default'] = X['default'].map({'no': 'No', 'unknown': 'Yes&Unknown', 'yes': 'Yes&Unknown'})\n",
    "\n",
    "    ## Feature engineering of other variables\n",
    "    ## pdays\n",
    "    conditions = [\n",
    "        (X['pdays'] == 999),\n",
    "        (X['pdays'] < 5),\n",
    "        ((X['pdays'] >= 5) & (X['pdays'] <= 10)),\n",
    "        (X['pdays'] > 10)\n",
    "    ]\n",
    "\n",
    "    choices = ['first visit', 'less than 5 days', '5 to 10 days', 'more than 10 days']\n",
    "\n",
    "    ## Create the 'pdays_bin' column based on conditions\n",
    "    X.loc[:,'pdays_bin'] = np.select(conditions, choices, default='unknown')\n",
    "\n",
    "    ## pmonths\n",
    "    conditions = [\n",
    "        (X['pmonths'] == 999),\n",
    "        (X['pmonths'] <= 0.2),\n",
    "        (X['pmonths'] > 0.2)\n",
    "    ]\n",
    "\n",
    "    choices = ['first visit', 'less than 2 months', 'more than 2 months']\n",
    "\n",
    "    ## Create the 'pmonths_bin' column based on conditions\n",
    "    X['pmonths_bin'] = np.select(conditions, choices, default='unknown')\n",
    "\n",
    "    ## drop pday and pmonth\n",
    "    X = X.drop(['pdays', 'pmonths'], axis=1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def feature_transformation(X):\n",
    "    # Drop target and unnecessary columns\n",
    "    x = X.drop(['responded'], axis=1)\n",
    "    y = X['responded']\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    X_encoded = pd.get_dummies(x, columns=['loan', 'marital', 'schooling', 'default', 'housing', 'day_of_week',\n",
    "                                           'poutcome', 'pdays_bin', 'pmonths_bin', 'profession', 'quarter', 'contact'], drop_first=True)\n",
    "\n",
    "    # Identify continuous columns for normalization\n",
    "    continuous_columns = ['custAge', 'campaign', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n",
    "                           'euribor3m', 'nr.employed', 'pastEmail']\n",
    "\n",
    "    # Extract the continuous columns from X_encoded\n",
    "    X_continuous = X_encoded[continuous_columns]\n",
    "\n",
    "    # Instantiate StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform the scaler on the continuous data\n",
    "    X_continuous_normalized = scaler.fit_transform(X_continuous)\n",
    "\n",
    "    # Replace the original continuous columns in X_encoded with the normalized ones\n",
    "    X_encoded[continuous_columns] = X_continuous_normalized\n",
    "\n",
    "    return X_encoded, y\n",
    "\n",
    "def train_and_evaluate_model(X_encoded, y):\n",
    "\n",
    "    #instantiate label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # encoding target for xgb \n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Apply SMOTEENN to the training data\n",
    "    smoteenn = SMOTEENN(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smoteenn.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Create a XGB classifier\n",
    "    xgb_clf = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "    # Train the classifier\n",
    "    xgb_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "\n",
    "    # Evaluate the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print('-'*50) # simple line\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # Display classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('-'*50) # simple line\n",
    "\n",
    "    # Get the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Predicted No', 'Predicted Yes'], \n",
    "                yticklabels=['Actual No', 'Actual Yes'])\n",
    "    plt.title('Confusion Matrix - XGB ')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return xgb_clf\n",
    "\n",
    "# Model and preprocessing file names\n",
    "model_file_name = \"propensify_model.joblib\"\n",
    "preprocessor_file_name = \"preprocessing_pipeline.joblib\"\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # Inbuilt Arguments\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Load data\n",
    "    train_df = pd.read_csv(\"...\")  # TODO: Paste the S3 path to your train.csv\n",
    "\n",
    "    # Create a preprocessing pipeline\n",
    "    preprocessing_pipeline = Pipeline([\n",
    "        ('missing_values', FunctionTransformer(func=treat_missing_values)),\n",
    "        ('label_encoding', FunctionTransformer(func=label_encoding)),\n",
    "        ('feature_transformation', FunctionTransformer(func=feature_transformation))\n",
    "    ])\n",
    "\n",
    "    # Fit transform the preprocessing pipeline\n",
    "    X_train_transformed, y_train = preprocessing_pipeline.fit_transform(train_df)\n",
    "\n",
    "    # Train and Evaluate model\n",
    "    xgb_clf =  train_and_evaluate_model(X_train_transformed, y_train)\n",
    "\n",
    "    # Save the preprocessing pipeline\n",
    "    preprocessor_save_path = os.path.join(args.model_dir, preprocessor_file_name)\n",
    "    joblib.dump(preprocessing_pipeline, preprocessor_save_path)\n",
    "    print(f\"Preprocessing pipeline saved at {preprocessor_save_path}\")\n",
    "\n",
    "    # Save the model\n",
    "    model_save_path = os.path.join(args.model_dir, model_file_name)\n",
    "    joblib.dump(xgb_clf, model_save_path)\n",
    "    print(f\"Model saved at {model_save_path}\")\n",
    "\n",
    "# Run the main function when the script runs\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "joblib\n",
    "scikit-learn\n",
    "xgboost\n",
    "imbalanced-learn\n",
    "matplotlib\n",
    "seaborn\n",
    "fsspec\n",
    "s3fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the training job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "# Choose instance_type: https://aws.amazon.com/sagemaker/pricing/\n",
    "# Choose framework_version: https://docs.aws.amazon.com/sagemaker/latest/dg/sklearn.html\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    base_job_name=\"xgb-pipeline-run\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    entry_point=\"train.py\",\n",
    "    dependencies=[\"requirements.txt\"],\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    use_spot_instances=True,\n",
    "    max_wait=600,\n",
    "    max_run=600,\n",
    "    role=get_execution_role(),\n",
    ")\n",
    "\n",
    "# Launch Training job\n",
    "sklearn_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "training_job_name = sklearn_estimator.latest_training_job.name\n",
    "\n",
    "# Obtain the location of the model stored on S3 - Optional\n",
    "# You can directly copy the location of the artifact from S3 also!\n",
    "model_artifact = sm_client.describe_training_job(\n",
    "    TrainingJobName=training_job_name\n",
    ")[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(f\"Training job name: {training_job_name}\")\n",
    "print(f\"Model storage location: {model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the inference script\n",
    "- Since the model has been trained with good accuracy we can deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serve.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile serve.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Load and return the model and preprocessing pipeline\"\"\"\n",
    "    model_file_name = \"propensify_model.joblib\"\n",
    "    preprocessor_file_name = \"preprocessing_pipeline.joblib\"\n",
    "\n",
    "    # Load the model\n",
    "    pipeline_model = joblib.load(os.path.join(model_dir, model_file_name))\n",
    "    \n",
    "    # Load the preprocessing pipeline\n",
    "    preprocessing_pipeline = joblib.load(os.path.join(model_dir, preprocessor_file_name))\n",
    "    \n",
    "    return pipeline_model, preprocessing_pipeline\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"Process the input json data and return the processed data.\n",
    "    You can also add any input data pre-processing in this function\n",
    "    \"\"\"\n",
    "    if request_content_type == \"application/json\":\n",
    "        input_object = pd.read_json(request_body, lines=True)\n",
    "        \n",
    "        return input_object\n",
    "    else:\n",
    "        raise ValueError(\"Only application/json content type supported!\")\n",
    "\n",
    "def predict_fn(input_object, model_and_pipeline):\n",
    "    \"\"\"Make predictions on processed input data\"\"\"\n",
    "\n",
    "    pipeline_model, preprocessing_pipeline = model_and_pipeline\n",
    "\n",
    "\n",
    "    # Apply the same preprocessing pipeline as in training\n",
    "    input_transformed , _ = preprocessing_pipeline.transform(input_object)\n",
    "\n",
    "    predictions = pipeline_model.predict(input_transformed)\n",
    "    pred_probs = pipeline_model.predict_proba(input_transformed)\n",
    "    \n",
    "    prediction_object = pd.DataFrame(\n",
    "        {\n",
    "            \"prediction\": predictions.tolist(),\n",
    "            \"pred_prob_class0\": pred_probs[:, 0].tolist(),\n",
    "            \"pred_prob_class1\": pred_probs[:, 1].tolist(),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return prediction_object\n",
    "\n",
    "def output_fn(prediction_object, request_content_type):\n",
    "    \"\"\"Post process the predictions and return as json\"\"\"\n",
    "    return_object = prediction_object.to_json(orient=\"records\", lines=True)\n",
    "    \n",
    "    return return_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Inference Endpoint\n",
    "- Most cost effective option for real time inference\n",
    "- Only runs when there is traffic so small delay in latency of first prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the deployment\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker import Session, get_execution_role\n",
    "\n",
    "session = Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "training_job_name = \"...\" # TODO: Update with  TrainingJobName \n",
    "model_artifact = f\"s3://{bucket}/{training_job_name}/output/model.tar.gz\"\n",
    "endpoint_name = \"propensify-xgb-pipeline-serverless\"\n",
    "\n",
    "model = SKLearnModel(\n",
    "    name=endpoint_name,\n",
    "    framework_version=\"1.2-1\",\n",
    "    entry_point=\"serve.py\",\n",
    "    dependencies=[\"requirements.txt\"],\n",
    "    model_data=model_artifact,\n",
    "    role=get_execution_role(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a config for serverless inference\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "serverless_config = ServerlessInferenceConfig(memory_size_in_mb=1024, max_concurrency=4)\n",
    "\n",
    "\n",
    "# Deploy!\n",
    "predictor = model.deploy(serverless_inference_config=serverless_config)\n",
    "endpoint_name = predictor.endpoint_name\n",
    "print(\"Endpoint name:\")\n",
    "print(f\"{endpoint_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some data that we want to make predictions on\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "test_df = pd.read_csv(\"...\") # TODO: Paste the S3 path to your test.csv\n",
    "test_df['responded'] = 99 # creating dummy target variable for preprocessing fit\n",
    "\n",
    "\n",
    "#  make predictions on\n",
    "X_pred = test_df.to_json(orient=\"records\", lines=True)\n",
    "\n",
    "# Submit to the endpoint\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response = sm_runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                      Body=X_pred, \n",
    "                                      ContentType=\"application/json\", \n",
    "                                      Accept=\"application/json\")\n",
    "# Decode the response from the endpoint\n",
    "response_body = response['Body']\n",
    "response_str = response_body.read().decode('utf-8')\n",
    "response_df = pd.read_json(response_str, lines=True)\n",
    "\n",
    "response_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results\n",
    "\n",
    "new_index = response_df['new_index'].tolist()\n",
    "response_df.index = new_index\n",
    "test_df['Predicted_Response']= response_df['Predicted_Response']\n",
    "\n",
    "test_df.to_csv(\"../data/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "- Delete the endpoint\n",
    "- Delete the endpoint config\n",
    "- Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
